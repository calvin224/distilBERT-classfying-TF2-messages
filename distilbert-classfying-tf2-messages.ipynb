{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q watermark\n!pip install matplotlib seaborn","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:28:32.106998Z","iopub.execute_input":"2023-04-04T21:28:32.107545Z","iopub.status.idle":"2023-04-04T21:28:53.394477Z","shell.execute_reply.started":"2023-04-04T21:28:32.107490Z","shell.execute_reply":"2023-04-04T21:28:53.393250Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.5.3)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.7/site-packages (0.12.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (9.4.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (4.38.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (23.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.4.4)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from seaborn) (4.4.0)\nRequirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.3.5)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.25->seaborn) (2022.7.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"%load_ext watermark\n%watermark -p torch,transformers,pandas","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:28:53.397869Z","iopub.execute_input":"2023-04-04T21:28:53.398296Z","iopub.status.idle":"2023-04-04T21:29:02.957925Z","shell.execute_reply.started":"2023-04-04T21:28:53.398253Z","shell.execute_reply":"2023-04-04T21:29:02.956755Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"torch       : 1.13.0\ntransformers: 4.26.1\npandas      : 1.3.5\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom tqdm.auto import tqdm\nimport seaborn as sns\nfrom transformers import AutoTokenizer, AutoModel\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:02.959613Z","iopub.execute_input":"2023-04-04T21:29:02.960646Z","iopub.status.idle":"2023-04-04T21:29:03.674714Z","shell.execute_reply.started":"2023-04-04T21:29:02.960605Z","shell.execute_reply":"2023-04-04T21:29:03.673700Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"# Model\nMODEL_CKPT = 'distilbert-base-uncased'\n\n# Hyperparameters\nMAX_LEN = 320\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = TRAIN_BATCH_SIZE * 2\nEPOCHS = 2\nLEARNING_RATE = 1e-05\nDEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(\"Device:\", DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:03.677411Z","iopub.execute_input":"2023-04-04T21:29:03.677697Z","iopub.status.idle":"2023-04-04T21:29:03.745691Z","shell.execute_reply.started":"2023-04-04T21:29:03.677670Z","shell.execute_reply":"2023-04-04T21:29:03.744537Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Device: cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"FOR_SUBMISSION = True  # `False` for experimenting","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:03.747154Z","iopub.execute_input":"2023-04-04T21:29:03.749031Z","iopub.status.idle":"2023-04-04T21:29:03.755103Z","shell.execute_reply.started":"2023-04-04T21:29:03.748992Z","shell.execute_reply":"2023-04-04T21:29:03.753745Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Load and Prepare Dataset","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv')\nprint(\"Num. samples:\", len(train_data))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:03.757193Z","iopub.execute_input":"2023-04-04T21:29:03.758154Z","iopub.status.idle":"2023-04-04T21:29:05.554753Z","shell.execute_reply.started":"2023-04-04T21:29:03.758114Z","shell.execute_reply":"2023-04-04T21:29:05.553499Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Num. samples: 159571\n","output_type":"stream"}]},{"cell_type":"code","source":"label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntrain_data['labels'] = train_data[label_columns].apply(lambda x: list(x), axis=1)\n\ntrain_data.drop(['id'], inplace=True, axis=1)\ntrain_data.drop(label_columns, inplace=True, axis=1)\n\ntrain_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:05.556450Z","iopub.execute_input":"2023-04-04T21:29:05.557259Z","iopub.status.idle":"2023-04-04T21:29:07.659362Z","shell.execute_reply.started":"2023-04-04T21:29:05.557213Z","shell.execute_reply":"2023-04-04T21:29:07.658194Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                        comment_text              labels\n0  Explanation\\nWhy the edits made under my usern...  [0, 0, 0, 0, 0, 0]\n1  D'aww! He matches this background colour I'm s...  [0, 0, 0, 0, 0, 0]\n2  Hey man, I'm really not trying to edit war. It...  [0, 0, 0, 0, 0, 0]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>[0, 0, 0, 0, 0, 0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def clean_text(txt):\n    \"\"\"Perform some basic cleaning of the text.\"\"\"\n    return re.sub(\"[^A-Za-z0-9.,;:!?]+\", ' ', str(txt))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:07.660944Z","iopub.execute_input":"2023-04-04T21:29:07.661402Z","iopub.status.idle":"2023-04-04T21:29:07.669033Z","shell.execute_reply.started":"2023-04-04T21:29:07.661363Z","shell.execute_reply":"2023-04-04T21:29:07.668019Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class MultiLabelDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len, new_data=False):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.text = dataframe.comment_text\n        self.new_data = new_data\n        self.max_len = max_len\n        \n        if not new_data:\n            self.targets = self.data.labels\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = \" \".join(text.split())\n        text = clean_text(text)\n\n        inputs = self.tokenizer(\n            text, \n            truncation=True, \n            padding='max_length' if self.new_data else False,\n            max_length=self.max_len, \n            return_tensors=\"pt\"\n        )\n        inputs = {k: v.squeeze() for k, v in inputs.items()}\n        \n        if not self.new_data:\n            labels = torch.tensor(self.targets[index], dtype=torch.float)\n            return inputs, labels\n\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:07.670796Z","iopub.execute_input":"2023-04-04T21:29:07.671167Z","iopub.status.idle":"2023-04-04T21:29:07.681913Z","shell.execute_reply.started":"2023-04-04T21:29:07.671132Z","shell.execute_reply":"2023-04-04T21:29:07.680833Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_size = 1.0 if FOR_SUBMISSION else 0.85\n\ntrain_df = train_data.sample(frac=train_size, random_state=123)\nval_df = train_data.drop(train_df.index).reset_index(drop=True)\ntrain_df = train_df.reset_index(drop=True)\n\nprint(\"Orig Dataset: {}\".format(train_data.shape))\nprint(\"Training Dataset: {}\".format(train_df.shape))\nprint(\"Validation Dataset: {}\".format(val_df.shape))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:07.687199Z","iopub.execute_input":"2023-04-04T21:29:07.687500Z","iopub.status.idle":"2023-04-04T21:29:07.754220Z","shell.execute_reply.started":"2023-04-04T21:29:07.687474Z","shell.execute_reply":"2023-04-04T21:29:07.753022Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Orig Dataset: (159571, 2)\nTraining Dataset: (159571, 2)\nValidation Dataset: (0, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT, do_lower_case=True)\n\ntrain_set = MultiLabelDataset(train_df, tokenizer, MAX_LEN)\nval_set = MultiLabelDataset(val_df, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:07.755805Z","iopub.execute_input":"2023-04-04T21:29:07.756189Z","iopub.status.idle":"2023-04-04T21:29:11.119195Z","shell.execute_reply.started":"2023-04-04T21:29:07.756151Z","shell.execute_reply":"2023-04-04T21:29:11.118175Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d698d7da833f470c97fa010b95fcd2cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69c92b7ad89841bbb35b1e24f5231a30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07ba72e0f0fa4d4e900d9b9642de7ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c66d81fbef94f3fa1201d3e20259cfa"}},"metadata":{}}]},{"cell_type":"markdown","source":"We've deliberately postponed padding until now, as we want to pad batches on the fly. Here is our custom data collator that will do that:","metadata":{}},{"cell_type":"code","source":"def dynamic_collate(data):\n    \"\"\"Custom data collator for dynamic padding.\"\"\"\n    inputs = [d for d,l in data]\n    labels = torch.stack([l for d,l in data], dim=0)\n    inputs = tokenizer.pad(inputs, return_tensors='pt')\n    return inputs, labels","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:11.120831Z","iopub.execute_input":"2023-04-04T21:29:11.121229Z","iopub.status.idle":"2023-04-04T21:29:11.127850Z","shell.execute_reply.started":"2023-04-04T21:29:11.121189Z","shell.execute_reply":"2023-04-04T21:29:11.126526Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 2, \n                'collate_fn': dynamic_collate}\n\nval_params = {'batch_size': VALID_BATCH_SIZE,\n              'shuffle': False,\n              'num_workers': 2, \n              'collate_fn': dynamic_collate}\n\ntrain_loader = DataLoader(train_set, **train_params)\nval_loader = None if FOR_SUBMISSION else DataLoader(val_set, **val_params)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:11.129145Z","iopub.execute_input":"2023-04-04T21:29:11.130194Z","iopub.status.idle":"2023-04-04T21:29:11.139362Z","shell.execute_reply.started":"2023-04-04T21:29:11.130156Z","shell.execute_reply":"2023-04-04T21:29:11.137963Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class TransformerModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(MODEL_CKPT)\n        self.classifier = nn.Sequential(\n            nn.Linear(768, 768),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(768, 6)\n        )\n\n    def forward(self, inputs):\n        bert_output = self.bert(**inputs)\n        hidden_state = bert_output.last_hidden_state\n        pooled_out = hidden_state[:, 0]\n        logits = self.classifier(pooled_out)\n        return logits\n\nmodel = TransformerModel()\nmodel.to(DEVICE);","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:11.141760Z","iopub.execute_input":"2023-04-04T21:29:11.142359Z","iopub.status.idle":"2023-04-04T21:29:19.421320Z","shell.execute_reply.started":"2023-04-04T21:29:11.142292Z","shell.execute_reply":"2023-04-04T21:29:19.420212Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a43e92c87ff4775becdf33c31a1a439"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"trainable_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\nprint(f\"Trainable params: {round(trainable_params/1e6, 1)} M\")","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:19.422929Z","iopub.execute_input":"2023-04-04T21:29:19.423695Z","iopub.status.idle":"2023-04-04T21:29:19.430626Z","shell.execute_reply.started":"2023-04-04T21:29:19.423652Z","shell.execute_reply":"2023-04-04T21:29:19.429502Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Trainable params: 67.0 M\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\nloss_func = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:19.432540Z","iopub.execute_input":"2023-04-04T21:29:19.432912Z","iopub.status.idle":"2023-04-04T21:29:19.442483Z","shell.execute_reply.started":"2023-04-04T21:29:19.432876Z","shell.execute_reply":"2023-04-04T21:29:19.441359Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"lr_sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.2)\ntest_df = pd.read_csv('/kaggle/input/tf2chatsunlabelled/chatlog.csv')\ntest_df.head(80)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:19.444204Z","iopub.execute_input":"2023-04-04T21:29:19.444569Z","iopub.status.idle":"2023-04-04T21:29:20.031825Z","shell.execute_reply.started":"2023-04-04T21:29:19.444533Z","shell.execute_reply":"2023-04-04T21:29:20.030758Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"             steamid          name comment_text  toxic  severe_toxic  obscene  \\\n0    [U:1:158517868]  Orange-Juice      bad spy    NaN           NaN      NaN   \n1    [U:1:158517868]  Orange-Juice           :(    NaN           NaN      NaN   \n2    [U:1:236710169]  CUNNY PLEASE         loll    NaN           NaN      NaN   \n3    [U:1:174480808]          kris           gg    NaN           NaN      NaN   \n4    [U:1:236710169]  CUNNY PLEASE       insane    NaN           NaN      NaN   \n..               ...           ...          ...    ...           ...      ...   \n75   [U:1:378260322]            AG           sb    NaN           NaN      NaN   \n76   [U:1:378260322]            AG           sb    NaN           NaN      NaN   \n77  [U:1:1210179367]         小灰灰本人            6    NaN           NaN      NaN   \n78  [U:1:1210179367]         小灰灰本人          .SS    NaN           NaN      NaN   \n79  [U:1:1210179367]         小灰灰本人          .SS    NaN           NaN      NaN   \n\n    threat  insult  identity_hate  \n0      NaN     NaN            NaN  \n1      NaN     NaN            NaN  \n2      NaN     NaN            NaN  \n3      NaN     NaN            NaN  \n4      NaN     NaN            NaN  \n..     ...     ...            ...  \n75     NaN     NaN            NaN  \n76     NaN     NaN            NaN  \n77     NaN     NaN            NaN  \n78     NaN     NaN            NaN  \n79     NaN     NaN            NaN  \n\n[80 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>steamid</th>\n      <th>name</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[U:1:158517868]</td>\n      <td>Orange-Juice</td>\n      <td>bad spy</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[U:1:158517868]</td>\n      <td>Orange-Juice</td>\n      <td>:(</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[U:1:236710169]</td>\n      <td>CUNNY PLEASE</td>\n      <td>loll</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[U:1:174480808]</td>\n      <td>kris</td>\n      <td>gg</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[U:1:236710169]</td>\n      <td>CUNNY PLEASE</td>\n      <td>insane</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>[U:1:378260322]</td>\n      <td>AG</td>\n      <td>sb</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>[U:1:378260322]</td>\n      <td>AG</td>\n      <td>sb</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>[U:1:1210179367]</td>\n      <td>小灰灰本人</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>[U:1:1210179367]</td>\n      <td>小灰灰本人</td>\n      <td>.SS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>[U:1:1210179367]</td>\n      <td>小灰灰本人</td>\n      <td>.SS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>80 rows × 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    \"\"\"An accuracy metric for multi-label problems.\"\"\"\n    if sigmoid: \n        inp = inp.sigmoid()\n    return ((inp > thresh) == targ.bool()).float().mean()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:20.033715Z","iopub.execute_input":"2023-04-04T21:29:20.034640Z","iopub.status.idle":"2023-04-04T21:29:20.041202Z","shell.execute_reply.started":"2023-04-04T21:29:20.034600Z","shell.execute_reply":"2023-04-04T21:29:20.039614Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(train_loader, model, loss_func, optimizer, progress_bar=None):\n    \"\"\"Train model over one epoch.\"\"\"\n    model.train()\n    size = len(train_loader.dataset)  # Train set size\n    \n    for i, (data, targets) in enumerate(train_loader):\n        # Put inputs and target on DEVICE\n        data = {k: v.to(DEVICE) for k, v in data.items()}\n        targets = targets.to(DEVICE)\n        \n        outputs = model(data)\n        loss = loss_func(outputs, targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if progress_bar is not None:\n            progress_bar.update(1)\n        \n        if i % 1000 == 0:\n            loss, step = loss.item(), i * len(targets)\n            print(f\"Loss: {loss:>4f}  [{step:>6d}/{size:>6d}]\")\n        elif i == len(train_loader) - 1:\n            loss = loss.item()\n            print(f\"Loss: {loss:>4f}  [{size:>6d}/{size:>6d}]\")","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:20.044931Z","iopub.execute_input":"2023-04-04T21:29:20.045278Z","iopub.status.idle":"2023-04-04T21:29:20.094817Z","shell.execute_reply.started":"2023-04-04T21:29:20.045250Z","shell.execute_reply":"2023-04-04T21:29:20.093555Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def validate_one_epoch(val_loader, model, loss_func):\n    \"\"\"Validate model over one epoch.\"\"\"\n    model.eval()\n    num_batches = len(val_loader)\n    \n    valid_loss, acc_multi = 0, 0\n\n    with torch.no_grad():\n        for _, (data, targets) in enumerate(val_loader):\n            data = {k: v.to(DEVICE) for k, v in data.items()}\n            targets = targets.to(DEVICE)\n\n            outputs = model(data)\n            valid_loss += loss_func(outputs, targets).item()\n            acc_multi += accuracy_multi(outputs, targets)\n\n    valid_loss /= num_batches  # Avg. loss\n    acc_multi /= num_batches   # Avg. acc. multi\n    print(f\"Avg. valid. loss: {valid_loss:>4f}, Acc. multi: {acc_multi:>4f}\\n\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:20.096346Z","iopub.execute_input":"2023-04-04T21:29:20.096803Z","iopub.status.idle":"2023-04-04T21:29:20.107803Z","shell.execute_reply.started":"2023-04-04T21:29:20.096743Z","shell.execute_reply":"2023-04-04T21:29:20.106811Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"num_train_steps = EPOCHS * len(train_loader)\nprogress_bar = tqdm(range(num_train_steps))\n\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1} (lr = {lr_sched.get_last_lr()[0]:.2e})\\n-------------------------------\")\n    train_one_epoch(train_loader, model, loss_func, optimizer, progress_bar)\n    if not FOR_SUBMISSION:\n        validate_one_epoch(val_loader, model, loss_func)\n    lr_sched.step()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T21:29:20.109264Z","iopub.execute_input":"2023-04-04T21:29:20.109853Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9974 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"645c995364cf47fca66114c5db020ad4"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 (lr = 1.00e-05)\n-------------------------------\n","output_type":"stream"},{"name":"stderr","text":"You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"Loss: 0.677642  [     0/159571]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Generate Test Submissions","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/tf2chatsunlabelled/chatlog.csv')\n\ntest_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_params = {'batch_size': VALID_BATCH_SIZE,\n               'shuffle': False,\n               'num_workers': 2}\n\ntest_set = MultiLabelDataset(test_df, tokenizer, MAX_LEN, new_data=True)\ntest_loader = DataLoader(test_set, **test_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(test_loader, model):\n    \"\"\"Make predictions on test set.\"\"\"\n    model.eval()\n    all_preds = []\n    \n    with torch.inference_mode():\n        for data in tqdm(test_loader):\n            data = {k: v.to(DEVICE) for k, v in data.items()}\n\n            outputs = model(data)\n            probas = torch.sigmoid(outputs)\n\n            all_preds.append(probas)\n            \n        all_preds = torch.cat(all_preds)\n    return all_preds.cpu()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_test_pred = predict(test_loader, model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_df = test_df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, name in enumerate(label_columns):\n    submit_df[name] = all_test_pred[:, i]\n\nsubmit_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsubmit_df.to_csv('Chatslabelled.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
